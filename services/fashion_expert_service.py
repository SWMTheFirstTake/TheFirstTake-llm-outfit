import asyncio
import openai
import logging
import anthropic
from typing import List, Dict
from config import settings
from models.fashion_models import FashionExpertType, ExpertAnalysisRequest

logger = logging.getLogger(__name__)

class SimpleFashionExpertService:
    def __init__(self):
        # self.client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        self.client = anthropic.Anthropic(api_key=settings.CLAUDE_API_KEY)  # ì¶”ê°€
        # ì „ë¬¸ê°€ë³„ íŠ¹ì„± ì •ì˜
        self.expert_profiles = {
            FashionExpertType.STYLE_ANALYST: {
                "role": "íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ë¶„ì„ ì „ë¬¸ê°€",
                "expertise": "ì²´í˜•ë¶„ì„, í•ê°ë¶„ì„, ì‹¤ë£¨ì—£",
                "focus": "ì‚¬ìš©ìì˜ ì²´í˜•ê³¼ ì–´ìš¸ë¦¬ëŠ” ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ê³  í•ê°ì„ ê³ ë ¤í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
                "prompt_template": "ì´ì „ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ì°¸ê³ í•˜ë˜, ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ì¸ ì²´í˜•ë¶„ì„ê³¼ í•ê° ê´€ì ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”. ë™ì˜í•  ìˆ˜ë„ ìˆê³ , ë‹¤ë¥¸ ê´€ì ì—ì„œ ìš°ë ¤ì‚¬í•­ì´ë‚˜ ëŒ€ì•ˆì„ ì œì‹œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì „ë¬¸ì  íŒë‹¨ì„ ì†”ì§í•˜ê²Œ í‘œí˜„í•˜ê³ , ì²´í˜•ê³¼ ì‹¤ë£¨ì—£ ê´€ì ì—ì„œ ì–´ìš¸ë¦¬ëŠ” ìŠ¤íƒ€ì¼ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ìƒ‰ìƒ(ë„¤ì´ë¹„, ë² ì´ì§€, í™”ì´íŠ¸, ì°¨ì½œ, ë¸”ë™, ê·¸ë ˆì´ ë“±), ì†Œì¬(ì½”íŠ¼, ë¦°ë„¨, ìš¸, ë°ë‹˜ ë“±), í•(ìŠ¬ë¦¼í•, ë ˆê·¤ëŸ¬í•, ì˜¤ë²„í• ë“±)ì„ í¬í•¨í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•˜ê³ , ë§ˆì§€ë§‰ì— ì¡°í•©ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ì¶”ê°€í•˜ì„¸ìš”. ì„±ë³„ì€ ëª¨ë¥´ê² ìœ¼ë©´ ë¬¼ì–´ë³´ê±°ë‚˜, 20ëŒ€ ë‚¨ìë¡œ ê°€ì •í•˜ì„¸ìš”."
            },
            FashionExpertType.TREND_EXPERT: {
                "role": "íŒ¨ì…˜ íŠ¸ë Œë“œ ì „ë¬¸ê°€",
                "expertise": "ìµœì‹ íŠ¸ë Œë“œ, ì…€ëŸ½ìŠ¤íƒ€ì¼",
                "focus": "ìµœì‹  íŒ¨ì…˜ íŠ¸ë Œë“œ, ì¸í”Œë£¨ì–¸ì„œ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
                "prompt_template": "ì´ì „ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ê³ ë ¤í•˜ë˜, íŠ¸ë Œë“œ ì „ë¬¸ê°€ë¡œì„œ ë‹¹ì‹ ë§Œì˜ ê´€ì ì„ ìœ ì§€í•˜ì„¸ìš”. í˜„ì¬ íŠ¸ë Œë“œì™€ ë§ì§€ ì•Šê±°ë‚˜ ì˜¤ë²„ëœ ìŠ¤íƒ€ì¼ì´ë¼ë©´ ì†”ì§í•˜ê²Œ ì§€ì í•˜ê³ , íŠ¸ë Œë””í•œ ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”. ë•Œë¡œëŠ” ì´ì „ ì˜ê²¬ì— ë™ì˜í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ íŠ¸ë Œë“œì™€ ì…€ëŸ½ ìŠ¤íƒ€ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ë‹¹ì‹ ì˜ ì†”ì§í•œ í‰ê°€ì™€ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ìƒ‰ìƒ(ë¼ë²¤ë”, ì„¸ì´ì§€ ê·¸ë¦°, í…Œë¼ì½”íƒ€, ë„¤ì´ë¹„, ë² ì´ì§€, í™”ì´íŠ¸ ë“±), ì†Œì¬(ì½”íŠ¼, ë¦°ë„¨, ìš¸, ë°ë‹˜ ë“±), í•(ìŠ¬ë¦¼í•, ë ˆê·¤ëŸ¬í•, ì˜¤ë²„í• ë“±)ì„ í¬í•¨í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•˜ê³ , ë§ˆì§€ë§‰ì— ì¡°í•©ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ì¶”ê°€í•˜ì„¸ìš”."
            },
            # FashionExpertType.BUDGET_MANAGER: {
            #     "role": "íŒ¨ì…˜ ì˜ˆì‚° ê´€ë¦¬ ì „ë¬¸ê°€", 
            #     "expertise": "ê°€ê²©ë¹„êµ, ê°€ì„±ë¹„, íˆ¬ìê°€ì¹˜",
            #     "focus": "ì˜ˆì‚° ëŒ€ë¹„ ìµœì ì˜ ê°€ì„±ë¹„ì™€ ì¥ê¸°ì  íˆ¬ìê°€ì¹˜ë¥¼ ê³ ë ¤í•œ í˜„ì‹¤ì ì¸ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
            #     "prompt_template": "ì˜ˆì‚°ì„ ê³ ë ¤í•˜ì—¬ ê°€ì„±ë¹„ ì¢‹ê³  íˆ¬ìê°€ì¹˜ê°€ ë†’ì€ ì‹¤ìš©ì ì¸ ì˜·ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”."
            # },
            FashionExpertType.COLOR_EXPERT: {
                "role": "í¼ìŠ¤ë„ ì»¬ëŸ¬ ì „ë¬¸ê°€",
                "expertise": "í¼ìŠ¤ë„ì»¬ëŸ¬, ìƒ‰ìƒì¡°í•©, í†¤ì˜¨í†¤", 
                "focus": "ê°œì¸ì˜ í”¼ë¶€í†¤ê³¼ ì–´ìš¸ë¦¬ëŠ” ìƒ‰ìƒ ë¶„ì„ê³¼ ì¡°í™”ë¡œìš´ ì»¬ëŸ¬ ì¡°í•©ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                "prompt_template": "ì´ì „ ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ì„ ì°¸ê³ í•˜ë˜, ì»¬ëŸ¬ ì „ë¬¸ê°€ë¡œì„œ ë‹¹ì‹ ì˜ ë…ë¦½ì ì¸ íŒë‹¨ì„ ìœ ì§€í•˜ì„¸ìš”. ìƒ‰ìƒ ì¡°í•©ì´ ë¶€ì ì ˆí•˜ê±°ë‚˜ í¼ìŠ¤ë„ ì»¬ëŸ¬ì™€ ë§ì§€ ì•ŠëŠ”ë‹¤ë©´ ì†”ì§í•˜ê²Œ ì§€ì í•˜ê³ , ë” ë‚˜ì€ ìƒ‰ìƒ ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”. ë•Œë¡œëŠ” ì´ì „ ì˜ê²¬ì— ë™ì˜í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í¼ìŠ¤ë„ ì»¬ëŸ¬ì™€ ìƒ‰ìƒ ì¡°í•© ê´€ì ì—ì„œ ë‹¹ì‹ ì˜ ì†”ì§í•œ í‰ê°€ì™€ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ìƒ‰ìƒ(ë„¤ì´ë¹„, ë² ì´ì§€, í™”ì´íŠ¸, ì°¨ì½œ, ë¸”ë™, ê·¸ë ˆì´, ë¼ë²¤ë”, ì„¸ì´ì§€ ê·¸ë¦° ë“±), ì†Œì¬(ì½”íŠ¼, ë¦°ë„¨, ìš¸, ë°ë‹˜ ë“±), í•(ìŠ¬ë¦¼í•, ë ˆê·¤ëŸ¬í•, ì˜¤ë²„í• ë“±)ì„ í¬í•¨í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•˜ê³ , ë§ˆì§€ë§‰ì— ì¡°í•©ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ì¶”ê°€í•˜ì„¸ìš”. ì„±ë³„ê³¼ í”¼ë¶€í†¤ì„ ì‚¬ìš©ìê°€ ì•Œë ¤ì£¼ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ì¼ë°˜ì ì¸ 20ëŒ€ ë‚¨ìë¡œ ê°€ì •í•˜ê³ , ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”."
            },
            # FashionExpertType.TPO_EXPERT: {
            #     "role": "TPO ìƒí™©ë³„ íŒ¨ì…˜ ì „ë¬¸ê°€",
            #     "expertise": "ìƒí™©ë¶„ì„, ë“œë ˆìŠ¤ì½”ë“œ, ì˜ˆì˜ë²”ì ˆ",
            #     "focus": "ì‹œê°„, ì¥ì†Œ, ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ë“œë ˆìŠ¤ì½”ë“œì™€ ì˜ˆì˜ë¥¼ ê³ ë ¤í•œ íŒ¨ì…˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
            #     "prompt_template": "ìƒí™©ê³¼ ì¥ì†Œì— ë§ëŠ” ì ì ˆí•œ ë“œë ˆìŠ¤ì½”ë“œë¥¼ ê³ ë ¤í•˜ì—¬ ì˜ˆì˜ì— ë§ëŠ” ì˜·ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”."
            # },
            FashionExpertType.FITTING_COORDINATOR: {
                "role": "ê°€ìƒ í”¼íŒ… ì½”ë””ë„¤ì´í„°",
                "expertise": "í”¼íŒ…ì—°ë™, ê²°ê³¼ë¶„ì„, ëŒ€ì•ˆì œì‹œ",
                "focus": "ëª¨ë“  ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì½”ë””ë„¤ì´ì…˜ì„ ì™„ì„±í•©ë‹ˆë‹¤.",
                "prompt_template": "ì•ì„  ì „ë¬¸ê°€ë“¤ì˜ ë‹¤ì–‘í•œ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… í‰ê°€ë¥¼ ë‚´ë ¤ì£¼ì„¸ìš”. ëª¨ë“  ì˜ê²¬ì´ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ê° ì „ë¬¸ê°€ì˜ ê´€ì ì„ ê³ ë ¤í•˜ë©´ì„œë„ ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ì½”ë””ë„¤ì´ì…˜ì„ ì œì•ˆí•´ì£¼ì„¸ìš”. ì˜ê²¬ì´ ì¶©ëŒí•˜ëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ê·¸ ì´ìœ ì™€ í•¨ê»˜ ìµœì¢… íŒë‹¨ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”. ìµœì¢… ì¶”ì²œì—ì„œëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ìƒ‰ìƒ(ë„¤ì´ë¹„, ë² ì´ì§€, í™”ì´íŠ¸, ì°¨ì½œ, ë¸”ë™, ê·¸ë ˆì´ ë“±), ì†Œì¬(ì½”íŠ¼, ë¦°ë„¨, ìš¸, ë°ë‹˜ ë“±), í•(ìŠ¬ë¦¼í•, ë ˆê·¤ëŸ¬í•, ì˜¤ë²„í• ë“±)ì„ í¬í•¨í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•˜ê³ , ë§ˆì§€ë§‰ì— ì¡°í•©ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ì¶”ê°€í•˜ì„¸ìš”."
            }
        }
    
    async def get_single_expert_analysis(self, request: ExpertAnalysisRequest):
        """ë‹¨ì¼ ì „ë¬¸ê°€ ë¶„ì„"""
        expert_profile = self.expert_profiles[request.expert_type]
        
        # ì „ë¬¸ê°€ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context_parts = [f"ì‚¬ìš©ì ìš”ì²­: {request.user_input}"]
        
        if request.user_profile:
            context_parts.append(f"ì‚¬ìš©ì ì •ë³´: {request.user_profile}")
        
        if request.context_info:
            context_parts.append(f"ìƒí™© ì •ë³´: {request.context_info}")
        
        context_parts.append(f"\n{expert_profile['role']}ìœ¼ë¡œì„œ {expert_profile['prompt_template']}")
        
        # êµ¬ì²´ì  í˜•ì‹ ê°•ì¡°
        context_parts.append("\nâš ï¸ ì¤‘ìš”: ì¶”ì²œí•  ë•Œ ë°˜ë“œì‹œ 'ìƒ‰ìƒ+ì†Œì¬+í•+ì•„ì´í…œëª…' í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.")
        context_parts.append("ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•˜ê³ , ë§ˆì§€ë§‰ì— ì¡°í•©ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        context_parts.append("ì˜ˆì‹œ: 'ë„¤ì´ë¹„ ì½”íŠ¼ ìŠ¬ë¦¼í• ì…”ì¸ ì™€ ë² ì´ì§€ ìš¸ ë ˆê·¤ëŸ¬í• íŒ¬ì¸ ë¥¼ ì¶”ì²œë“œë ¤ìš”. ê¹”ë”í•˜ë©´ì„œë„ ì„¸ë ¨ëœ ëŠë‚Œì„ ì¤„ ìˆ˜ ìˆì–´ìš”.'")
        context_parts.append("ì¶”ìƒì ì¸ í‘œí˜„(ì˜ˆ: 'ìŠ¬ë¦¼í• ì…”ì¸ ', 'ë‹¤í¬ ì§„')ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.")
        
        expert_prompt = "\n\n".join(context_parts)
        
        # OpenAI í˜¸ì¶œ
        try:
            system_prompt = f"""ë‹¹ì‹ ì€ {expert_profile['role']}ì…ë‹ˆë‹¤. {expert_profile['focus']} ì „ë¬¸ ì˜ì—­: {expert_profile['expertise']}

ì¤‘ìš”í•œ ì›ì¹™:
1. ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ì— ëŒ€í•œ ë…ë¦½ì ì¸ íŒë‹¨ì„ ìœ ì§€í•˜ì„¸ìš”
2. ì´ì „ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì— ë¬´ì¡°ê±´ ë™ì˜í•˜ì§€ ë§ˆì„¸ìš”
3. ë‹¹ì‹ ì˜ ê´€ì ì—ì„œ ë¬¸ì œì ì´ë‚˜ ìš°ë ¤ì‚¬í•­ì´ ìˆë‹¤ë©´ ì†”ì§í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”
4. ë•Œë¡œëŠ” ì´ì „ ì˜ê²¬ê³¼ ë‹¤ë¥¸ ëŒ€ì•ˆì„ ì œì‹œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
5. ë‹¹ì‹ ì˜ ì „ë¬¸ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì†”ì§í•˜ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”

êµ¬ì²´ì  ì •ë³´ í¬í•¨ í•„ìˆ˜:
- ë°˜ë“œì‹œ ìƒ‰ìƒì„ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: ë„¤ì´ë¹„, ë² ì´ì§€, í™”ì´íŠ¸, ì°¨ì½œ, ë¸”ë™, ê·¸ë ˆì´ ë“±)
- ì†Œì¬ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: ì½”íŠ¼, ë¦°ë„¨, ìš¸, ë°ë‹˜, ì‹¤í¬ ë“±)
- í•ì„ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: ìŠ¬ë¦¼í•, ë ˆê·¤ëŸ¬í•, ì˜¤ë²„í•, ë£¨ì¦ˆí• ë“±)
- êµ¬ì²´ì ì¸ ì•„ì´í…œëª…ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: "ìŠ¬ë¦¼í• ì…”ì¸ " ëŒ€ì‹  "ë„¤ì´ë¹„ ì½”íŠ¼ ìŠ¬ë¦¼í• ì…”ì¸ ")

ì‘ë‹µ í˜•ì‹:
- ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”
- "ìƒ‰ìƒ+ì†Œì¬+í•+ì•„ì´í…œëª…" í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„
- ë§ˆì§€ë§‰ì— ì¡°í•©ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ì¶”ê°€í•˜ì„¸ìš”
- ì˜ˆì‹œ: "ë„¤ì´ë¹„ ì½”íŠ¼ ìŠ¬ë¦¼í• ì…”ì¸ ì™€ ë² ì´ì§€ ìš¸ ë ˆê·¤ëŸ¬í• íŒ¬ì¸ ë¥¼ ì¶”ì²œë“œë ¤ìš”. ê¹”ë”í•˜ë©´ì„œë„ ì„¸ë ¨ëœ ëŠë‚Œì„ ì¤„ ìˆ˜ ìˆì–´ìš”."

ì¶”ìƒì ì¸ í‘œí˜„ ê¸ˆì§€:
- "ì˜ˆìœ ì˜·", "ë©‹ì§„ ìŠ¤íƒ€ì¼" ê°™ì€ ì¶”ìƒì  í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
- ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ìƒ‰ìƒ, ì†Œì¬, í• ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤"""
            
            response = await self._call_openai_async(
                system_prompt,
                expert_prompt
            )
            
            return {
                "expert_type": request.expert_type.value,
                "expert_role": expert_profile["role"],
                "analysis": response,
                "expertise_areas": expert_profile["expertise"]
            }
            
        except Exception as e:
            logger.error(f"ì „ë¬¸ê°€ ë¶„ì„ ì‹¤íŒ¨ - {request.expert_type}: {e}")
            raise e
    
    async def get_expert_chain_analysis(self, request):
        """ì „ë¬¸ê°€ ì²´ì¸ ë¶„ì„"""
        expert_results = []
        accumulated_insights = []
        
        for expert_type in request.expert_sequence or []:
            # ì´ì „ ì „ë¬¸ê°€ë“¤ì˜ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
            current_context = request.context_info or {}
            if accumulated_insights:
                current_context["previous_expert_insights"] = accumulated_insights[-3:]  # ìµœê·¼ 3ê°œë§Œ
            
            expert_request = ExpertAnalysisRequest(
                user_input=request.user_input,
                room_id=request.room_id,
                expert_type=expert_type,
                user_profile=request.user_profile,
                context_info=current_context
            )
            
            expert_result = await self.get_single_expert_analysis(expert_request)
            expert_results.append(expert_result)
            
            # ë‹¤ìŒ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸ ëˆ„ì 
            accumulated_insights.append({
                "expert": expert_type.value,
                "key_point": expert_result["analysis"][:100] + "..."  # ìš”ì•½ë§Œ
            })
        
        # ìµœì¢… ì¢…í•©
        return {
            "expert_analyses": expert_results,
            # "expert_count": len(expert_results),
            # "comprehensive_recommendation": self._synthesize_results(expert_results),
            # "collaboration_flow": accumulated_insights
        }
    
    def _synthesize_results(self, expert_results: List[Dict]) -> str:
        """ì „ë¬¸ê°€ ê²°ê³¼ ì¢…í•©"""
        synthesis = "===== ì¢…í•© íŒ¨ì…˜ ì¶”ì²œ =====\n\n"
        
        for result in expert_results:
            synthesis += f"ğŸ”¹ {result['expert_role']}: {result['analysis'][:150]}...\n\n"
        
        synthesis += "ğŸ“‹ ìµœì¢… ì¶”ì²œ: ëª¨ë“  ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ ì¢…í•©í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë‹¨ í•˜ë‚˜ì˜ ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ëŒ€ì•ˆ ì—†ì´."
        
        return synthesis
    
    async def _call_openai_async(self, system_prompt: str, user_prompt: str) -> str:
        """ë¹„ë™ê¸° OpenAI í˜¸ì¶œ"""
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            self._call_openai_sync,
            system_prompt,
            user_prompt
        )
        return response
    
    def _call_openai_sync(self, system_prompt: str, user_prompt: str) -> str:
        # """ë™ê¸° OpenAI í˜¸ì¶œ"""
        # response = self.client.chat.completions.create(
        #     model=settings.LLM_MODEL_NAME,
        #     messages=[
        #         {"role": "system", "content": system_prompt},
        #         {"role": "user", "content": user_prompt}
        #     ],
        #     max_tokens=settings.LLM_MAX_TOKENS,
        #     temperature=settings.LLM_TEMPERATURE
        # )
        # content = response.choices[0].message.content
        # if content is None:
        #     return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        # return content 
        """Claude API í˜¸ì¶œë¡œ ë³€ê²½"""
        response = self.client.messages.create(
            model=settings.LLM_MODEL_NAME,
            max_tokens=settings.LLM_MAX_TOKENS,
            temperature=settings.LLM_TEMPERATURE,
            system=system_prompt,  # ClaudeëŠ” system íŒŒë¼ë¯¸í„° ì‚¬ìš©
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        content = response.content[0].text  # Claude ì‘ë‹µ êµ¬ì¡°
        if content is None:
            return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return content
        