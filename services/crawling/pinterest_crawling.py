from selenium import webdriver
from selenium.webdriver.common.by import By
import time
import requests
from bs4 import BeautifulSoup
import json
import random
import os

class ImprovedPinterestScraper:
    def __init__(self):
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(options=options)
        # ë´‡ ê°ì§€ ë°©ì§€
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
    def search_pins_improved(self, query, max_pins=50):
        search_url = f"https://www.pinterest.com/search/pins/?q={query}"
        print(f"ê²€ìƒ‰ ì‹œì‘: {search_url}")
        
        self.driver.get(search_url)
        time.sleep(5)  # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
        
        pins_data = []
        seen_urls = set()  # ì¤‘ë³µ ë°©ì§€
        scroll_attempts = 0
        max_scroll_attempts = 20
        no_new_content_count = 0
        
        while len(pins_data) < max_pins and scroll_attempts < max_scroll_attempts:
            # í˜„ì¬ í˜ì´ì§€ ë†’ì´ ê¸°ë¡
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ëœë¤ ëŒ€ê¸° (ë´‡ ê°ì§€ ë°©ì§€)
            time.sleep(random.uniform(3, 5))
            
            # ì¶”ê°€ ìŠ¤í¬ë¡¤ (Pinterest íŠ¹ì„±ìƒ ì—¬ëŸ¬ ë²ˆ ìŠ¤í¬ë¡¤ í•„ìš”)
            for i in range(3):
                self.driver.execute_script("window.scrollBy(0, 300);")
                time.sleep(1)
            
            # í•€ ìš”ì†Œë“¤ ì°¾ê¸°
            pin_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-test-id="pin"]')
            print(f"ìŠ¤í¬ë¡¤ {scroll_attempts+1}: í˜ì´ì§€ì—ì„œ {len(pin_elements)}ê°œ í•€ ë°œê²¬")
            
            # ìƒˆë¡œìš´ í•€ ìˆ˜ì§‘
            new_pins_in_this_scroll = 0
            for pin in pin_elements:
                if len(pins_data) >= max_pins:
                    break
                    
                try:
                    img_element = pin.find_element(By.TAG_NAME, "img")
                    img_url = img_element.get_attribute("src")
                    
                    # ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸
                    if img_url and img_url.startswith('https') and img_url not in seen_urls:
                        img_alt = img_element.get_attribute("alt") or "No description"
                        
                        # Pinterest ê´‘ê³ ë‚˜ í”„ë¡œëª¨ì…˜ ì´ë¯¸ì§€ í•„í„°ë§
                        if "promoted" in img_alt.lower() or "ad" in img_alt.lower():
                            continue
                            
                        try:
                            pin_link = pin.find_element(By.TAG_NAME, "a").get_attribute("href")
                        except:
                            pin_link = "No link"
                        
                        pin_data = {
                            "image_url": img_url,
                            "description": img_alt,
                            "pin_url": pin_link,
                            "query": query,
                            "collected_at": time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        pins_data.append(pin_data)
                        seen_urls.add(img_url)
                        new_pins_in_this_scroll += 1
                        
                        print(f"ìˆ˜ì§‘ë¨ {len(pins_data)}: {img_alt[:50]}...")
                        
                except Exception as e:
                    continue
            
            # ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_pins_in_this_scroll == 0:
                no_new_content_count += 1
                print(f"ìƒˆ í•€ ì—†ìŒ ({no_new_content_count}ë²ˆì§¸)")
                
                if no_new_content_count >= 3:
                    print("ì—°ì†ìœ¼ë¡œ ìƒˆ ì½˜í…ì¸ ê°€ ì—†ì–´ì„œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
            else:
                no_new_content_count = 0  # ìƒˆ ì½˜í…ì¸  ë°œê²¬ì‹œ ë¦¬ì…‹
                
            scroll_attempts += 1
            
            # í˜ì´ì§€ ë†’ì´ê°€ ë³€í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€ ëŒ€ê¸°
            if new_height == last_height:
                print("í˜ì´ì§€ ë†’ì´ ë³€í™” ì—†ìŒ. ì¶”ê°€ ëŒ€ê¸°...")
                time.sleep(3)
        
        print(f"ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ: {len(pins_data)}ê°œ")
        return pins_data
    
    def multi_query_search(self, queries, pins_per_query=15):
        """ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ ë¶„ì‚° ìˆ˜ì§‘"""
        all_pins = []
        all_seen_urls = set()
        
        for i, query in enumerate(queries):
            print(f"\n=== ê²€ìƒ‰ì–´ {i+1}/{len(queries)}: '{query}' ===")
            
            pins = self.search_pins_improved(query, pins_per_query)
            
            # ì „ì²´ì ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            unique_pins = []
            for pin in pins:
                if pin['image_url'] not in all_seen_urls:
                    unique_pins.append(pin)
                    all_seen_urls.add(pin['image_url'])
            
            all_pins.extend(unique_pins)
            print(f"'{query}'ì—ì„œ {len(unique_pins)}ê°œ ê³ ìœ  í•€ ìˆ˜ì§‘ (ì¤‘ë³µ {len(pins) - len(unique_pins)}ê°œ ì œê±°)")
            
            # ê²€ìƒ‰ì–´ ê°„ ë”œë ˆì´ (ë´‡ ê°ì§€ ë°©ì§€)
            if i < len(queries) - 1:
                wait_time = random.uniform(8, 12)
                print(f"{wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
        
        return all_pins
    
    def close(self):
        self.driver.quit()

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    scraper = ImprovedPinterestScraper()
    
    try:
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ ë¶„ì‚° ìˆ˜ì§‘ (ë‚¨ì„± ì „ìš©)
        search_queries = [
            # "korean men summer fashion",      # ì˜ì–´ ê¸°ë³¸
            # "men summer outfit korean",       # ì˜ì–´ ë³€í˜•
            # "korean summer street style",     # ìŠ¤íŠ¸ë¦¿ ìŠ¤íƒ€ì¼
            # "korean casual summer men",       # ìºì£¼ì–¼ ê°•ì¡°
            # "men summer fashion seoul"        # ì„œìš¸ íŒ¨ì…˜
            # "korean men summer style",        # ìŠ¤íƒ€ì¼ ê°•ì¡°
            # "men summer fashion korea",       # í•œêµ­ íŒ¨ì…˜
            # "korean summer men outfit",       # ì•„ì›ƒí• ê°•ì¡°
            # "korean men casual summer",       # ìºì£¼ì–¼ ì—¬ë¦„
            # "men summer street fashion korean", # ìŠ¤íŠ¸ë¦¿ íŒ¨ì…˜
            # "korean men summer looks",        # ë£© ê°•ì¡°
            # "men summer style korean fashion", # íŒ¨ì…˜ ìŠ¤íƒ€ì¼
            # "korean summer men clothing",     # ì˜ë¥˜ ê°•ì¡°
            # "men summer outfit ideas korean", # ì•„ì›ƒí• ì•„ì´ë””ì–´
            # "korean men summer fashion trends", # íŠ¸ë Œë“œ
            # "men summer casual korean style", # ìºì£¼ì–¼ ìŠ¤íƒ€ì¼
            # "korean summer men streetwear",   # ìŠ¤íŠ¸ë¦¿ì›¨ì–´
            # "men summer fashion inspiration korean", # ì˜ê°
            # "korean men summer wardrobe",     # ì›Œë“œë¡œë¸Œ
            # "men summer style inspiration korean", # ìŠ¤íƒ€ì¼ ì˜ê°
            # "korean men summer fashion male", # ë‚¨ì„± ëª…ì‹œ
            # "men summer outfit korean male",  # ë‚¨ì„± ì•„ì›ƒí•
            # "korean men summer clothing male", # ë‚¨ì„± ì˜ë¥˜
            # "men summer style korean male",   # ë‚¨ì„± ìŠ¤íƒ€ì¼
            # "korean men summer fashion guy",  # ë‚¨ì„± íŒ¨ì…˜
            # "men summer outfit ideas korean male", # ë‚¨ì„± ì•„ì´ë””ì–´
            # "korean men summer fashion boy",  # ë‚¨ì„± íŒ¨ì…˜
            # "men summer casual korean male",  # ë‚¨ì„± ìºì£¼ì–¼
            # "korean men summer street style male", # ë‚¨ì„± ìŠ¤íŠ¸ë¦¿
            # "korean men summer fashion male", # ë‚¨ì„± ëª…ì‹œ
            # "men summer outfit korean male",  # ë‚¨ì„± ì•„ì›ƒí•
            # "korean men summer clothing male", # ë‚¨ì„± ì˜ë¥˜
            # "men summer style korean male",   # ë‚¨ì„± ìŠ¤íƒ€ì¼
            # "korean men summer fashion guy",   # ë‚¨ì„± íŒ¨ì…˜
            "korea summer street style",        # í•œêµ­ ì—¬ë¦„ ìŠ¤íŠ¸ë¦¿
            "korea summer business casual",     # í•œêµ­ ì—¬ë¦„ ë¹„ì¦ˆë‹ˆìŠ¤ ìºì£¼ì–¼
            "korea summer streetwear",          # í•œêµ­ ì—¬ë¦„ ìŠ¤íŠ¸ë¦¿ì›¨ì–´
            "korea summer casual style",        # í•œêµ­ ì—¬ë¦„ ìºì£¼ì–¼
            "korea summer fashion trends"       # í•œêµ­ ì—¬ë¦„ íŒ¨ì…˜ íŠ¸ë Œë“œ
        ]
        
        print("=== ë‹¤ì¤‘ ê²€ìƒ‰ì–´ë¡œ íŒ¨ì…˜ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘ ===")
        all_pins = scraper.multi_query_search(search_queries, pins_per_query=30)
        
        print(f"\nğŸ‰ ì´ {len(all_pins)}ê°œì˜ ê³ ìœ í•œ í•€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¶”ê°€)
        output_file = "korean_mens_summer_fashion_pinterest.json"
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        existing_pins = []
        if os.path.exists(output_file):
            try:
                with open(output_file, "r", encoding="utf-8") as f:
                    existing_pins = json.load(f)
                print(f"ğŸ“ ê¸°ì¡´ íŒŒì¼ì—ì„œ {len(existing_pins)}ê°œ ë°ì´í„° ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                existing_pins = []
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê¸°ì¡´ URL ì¶”ì¶œ
        existing_urls = set(pin['image_url'] for pin in existing_pins)
        
        # ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ì œê±°
        new_unique_pins = []
        for pin in all_pins:
            if pin['image_url'] not in existing_urls:
                new_unique_pins.append(pin)
                existing_urls.add(pin['image_url'])
        
        # ê¸°ì¡´ + ìƒˆë¡œìš´ ë°ì´í„° í•©ì¹˜ê¸°
        combined_pins = existing_pins + new_unique_pins
        
        # íŒŒì¼ì— ì €ì¥
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(combined_pins, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(new_unique_pins)}ê°œ ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€ë¨ (ì´ {len(combined_pins)}ê°œ)")
        
        print(f"ê²°ê³¼ê°€ {output_file} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²€ìƒ‰ì–´ë³„ í†µê³„
        query_stats = {}
        for pin in all_pins:
            query = pin['query']
            query_stats[query] = query_stats.get(query, 0) + 1
        
        print("\nğŸ“Š ê²€ìƒ‰ì–´ë³„ ìˆ˜ì§‘ í†µê³„:")
        for query, count in query_stats.items():
            print(f"  - {query}: {count}ê°œ")
        
        # ì²˜ìŒ 5ê°œ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ–¼ï¸  ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ê°œ):")
        for i, pin in enumerate(all_pins[:5]):
            print(f"{i+1}. {pin['description'][:60]}...")
            print(f"   ì´ë¯¸ì§€: {pin['image_url']}")
            print(f"   ì¶œì²˜: {pin['query']}")
            print()
            
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close()
        print("ë¸Œë¼ìš°ì € ì¢…ë£Œë¨")