# from fastapi import FastAPI
# from pydantic import BaseModel
# from services.gpt_service import ask_gpt
# from typing import Generic, TypeVar, Optional, List
# from pydantic.generics import GenericModel
# import re

# # ê³µí†µ ì‘ë‹µ ëª¨ë¸ ì •ì˜
# T = TypeVar("T")

# class ResponseModel(BaseModel, Generic[T]):
#     status: str = "success"
#     message: str = "Operation was successful."
#     data: Optional[T]  # ì œë„ˆë¦­ íƒ€ì…ìœ¼ë¡œ ë°ì´í„° ì •ì˜

# # ì‹¤ì œ ì‘ë‹µì— ë“¤ì–´ê°ˆ ë°ì´í„° ëª¨ë¸
# class AskResponseData(BaseModel):
#     question: str
#     options: List[str]  # ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥

# # ì‹¤ì œ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
# app = FastAPI()

# class PromptRequest(BaseModel):
#     prompt: str

# # ì§ˆë¬¸ê³¼ ì‘ë‹µì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
# def split_question_and_answers(generated_text: str):
#     # ì „ì²´ ì¤„ì„ ë¶„ë¦¬
#     lines = generated_text.strip().splitlines()

#     question = ""
#     options = []

#     # 1. ë¨¼ì € ì§ˆë¬¸ ì°¾ê¸° (Q. ë¡œ ì‹œì‘í•˜ëŠ” ì¤„)
#     for line in lines:
#         if line.strip().startswith("Q."):
#             question = line.strip().replace("Q.", "").strip()
#             break  # ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ ì¶”ì¶œ

#     # 2. ê·¸ ì™¸ì˜ ì¤„ ì¤‘ì—ì„œ A., B., C. ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì„ ë‹µë³€ìœ¼ë¡œ ì¶”ì¶œ
#     for line in lines:
#         line = line.strip()
#         match = re.match(r"^[A-PR-Z]\.\s*(.*)", line)  # Q ì œì™¸
#         if match:
#             options.append(match.group(1).strip())

#     return question, options
    
# @app.post("/api/ask")
# def ask(request: PromptRequest):
#     generated_text = ask_gpt(request.prompt)
#     return ResponseModel(data=generated_text)

# @app.get("/api/test")
# def test():
#     return {"message": "Hello, World!"}
# app = FastAPI(title="Fashion Curation API", version="2.0.0")

# # ë¼ìš°í„° ë“±ë¡
# app.include_router(curation_router)

# # main_simple.py - Redis ì—†ì´ ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš©
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# from typing import List, Optional, Any
# from enum import Enum
# import asyncio
# import openai
# from config import settings
# import logging

# app = FastAPI(title="Fashion Curation API", version="2.0.0")

# # ëª¨ë¸ ì •ì˜
# class ResponseModel(BaseModel):
#     success: bool = True
#     message: str = "Success"
#     data: Any = None

# class CurationStyle(str, Enum):
#     FORMAL = "formal"
#     CASUAL = "casual"
#     CITY_BOY = "city_boy"

# class CurationRequest(BaseModel):
#     user_input: str
#     room_id: int
#     styles: Optional[List[CurationStyle]] = [CurationStyle.FORMAL, CurationStyle.CASUAL, CurationStyle.CITY_BOY]

# class PromptRequest(BaseModel):
#     prompt: str

# # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € (ë©”ëª¨ë¦¬ ê¸°ë°˜)
# class SimplePromptManager:
#     def __init__(self):
#         self.contexts = {}  # ë©”ëª¨ë¦¬ì— ì €ì¥
#         self.style_templates = {
#             CurationStyle.FORMAL: "ë¼ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì›¬ë§Œí•˜ë©´ í¬ë©€í•œ ìŠ¤íƒ€ì¼ë¡œ ìŠ¤íƒ€ì¼ í•œ ê°œë§Œ ì¶”ì²œí•´ì¤˜",
#             CurationStyle.CASUAL: "ë¼ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì›¬ë§Œí•˜ë©´ ìºì¥¬ì–¼í•œ ìŠ¤íƒ€ì¼ë¡œ ìŠ¤íƒ€ì¼ í•œ ê°œë§Œ ì¶”ì²œí•´ì¤˜",
#             CurationStyle.CITY_BOY: "ë¼ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì›¬ë§Œí•˜ë©´ ì‹œí‹°ë³´ì´ ìŠ¤íƒ€ì¼ë¡œ ìŠ¤íƒ€ì¼ í•œ ê°œë§Œ ì¶”ì²œí•´ì¤˜"
#         }
    
#     def get_or_create_prompt_context(self, room_id: int, user_input: str) -> str:
#         existing_context = self.contexts.get(room_id, "")
#         if existing_context:
#             return existing_context + "\n" + user_input
#         else:
#             return user_input
    
#     def build_style_prompt(self, context: str, style: CurationStyle) -> str:
#         style_addition = self.style_templates.get(style, "")
#         return context + style_addition
    
#     def update_prompt_context(self, room_id: int, new_context: str):
#         self.contexts[room_id] = new_context
    
#     def clear_context(self, room_id: int):
#         self.contexts.pop(room_id, None)

# # ê°„ë‹¨í•œ íë ˆì´ì…˜ ì„œë¹„ìŠ¤
# class SimpleCurationService:
#     def __init__(self):
#         self.client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
#         self.prompt_manager = SimplePromptManager()
    
#     async def generate_curation_response(self, request: CurationRequest):
#         current_context = self.prompt_manager.get_or_create_prompt_context(
#             request.room_id, 
#             request.user_input
#         )
        
#         tasks = []
#         for style in request.styles:
#             task = self._generate_single_curation(current_context, style)
#             tasks.append(task)
        
#         curation_results = await asyncio.gather(*tasks, return_exceptions=True)
        
#         results = []
#         context_builder = [current_context]
        
#         for i, (style, result) in enumerate(zip(request.styles, curation_results)):
#             if isinstance(result, Exception):
#                 content = "íë ˆì´ì…˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
#             else:
#                 content = result + f" ({i+1}ë²ˆì§¸ AI)"
#                 context_builder.append(content)
            
#             results.append({
#                 "style": style.value,
#                 "content": content
#             })
        
#         updated_context = '\n'.join(context_builder)
#         self.prompt_manager.update_prompt_context(request.room_id, updated_context)
        
#         return {
#             "room_id": request.room_id,
#             "results": results
#         }
    
#     async def _generate_single_curation(self, context: str, style: CurationStyle) -> str:
#         try:
#             style_prompt = self.prompt_manager.build_style_prompt(context, style)
            
#             loop = asyncio.get_event_loop()
#             response = await loop.run_in_executor(
#                 None, 
#                 self._call_openai_sync,
#                 style_prompt
#             )
            
#             return response
#         except Exception as e:
#             logging.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
#             raise e
    
#     def _call_openai_sync(self, user_prompt: str) -> str:
#         response = self.client.chat.completions.create(
#             model=settings.LLM_MODEL_NAME,
#             messages=[
#                 {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ìƒ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
#                 {"role": "user", "content": user_prompt}
#             ],
#             max_tokens=settings.LLM_MAX_TOKENS,
#             temperature=settings.LLM_TEMPERATURE
#         )
#         return response.choices[0].message.content

# # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
# curation_service = SimpleCurationService()

# # API ì—”ë“œí¬ì¸íŠ¸
# @app.post("/api/curation")
# async def generate_curation(request: CurationRequest):
#     try:
#         result = await curation_service.generate_curation_response(request)
#         return ResponseModel(data=result)
#     except Exception as e:
#         logging.error(f"íë ˆì´ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.post("/api/ask")
# def ask_single(request: PromptRequest):
#     try:
#         response = curation_service.client.chat.completions.create(
#             model=settings.LLM_MODEL_NAME,
#             messages=[
#                 {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ìƒ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
#                 {"role": "user", "content": request.prompt}
#             ],
#             max_tokens=settings.LLM_MAX_TOKENS,
#             temperature=settings.LLM_TEMPERATURE
#         )
#         return ResponseModel(data=response.choices[0].message.content)
#     except Exception as e:
#         logging.error(f"ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤íŒ¨: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.get("/api/test")
# def test():
#     return {"message": "Hello, World!"}

# @app.get("/health")
# def health_check():
#     return {"status": "healthy"}

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run("main_simple:app", host="0.0.0.0", port=6020, reload=True)
# main_simple_experts.py - ì „ë¬¸ê°€ ì‹œìŠ¤í…œ ì ìš©
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Any, Dict
from enum import Enum
import asyncio
import openai
from config import settings
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Fashion Expert API", version="2.0.0")

# ëª¨ë¸ ì •ì˜
class ResponseModel(BaseModel):
    success: bool = True
    message: str = "Success"
    data: Any = None

class FashionExpertType(str, Enum):
    STYLE_ANALYST = "style_analyst"          # ìŠ¤íƒ€ì¼ ë¶„ì„ê°€
    TREND_EXPERT = "trend_expert"            # íŠ¸ë Œë“œ ì „ë¬¸ê°€
    # BUDGET_MANAGER = "budget_manager"        # ì˜ˆì‚° ê´€ë¦¬ì
    COLOR_EXPERT = "color_expert"            # ìƒ‰ìƒ ì „ë¬¸ê°€
    # TPO_EXPERT = "tpo_expert"               # TPO ì „ë¬¸ê°€
    FITTING_COORDINATOR = "fitting_coordinator"  # ê°€ìƒí”¼íŒ… ì½”ë””ë„¤ì´í„°

class ExpertAnalysisRequest(BaseModel):
    user_input: str
    room_id: int
    expert_type: FashionExpertType
    user_profile: Optional[Dict] = None
    context_info: Optional[Dict] = None

class ExpertChainRequest(BaseModel):
    user_input: str
    room_id: int
    expert_sequence: Optional[List[FashionExpertType]] = [
        FashionExpertType.STYLE_ANALYST,
        FashionExpertType.COLOR_EXPERT, 
        FashionExpertType.TREND_EXPERT,
        # FashionExpertType.TPO_EXPERT,
        # FashionExpertType.BUDGET_MANAGER,
        FashionExpertType.FITTING_COORDINATOR
    ]
    user_profile: Optional[Dict] = None
    context_info: Optional[Dict] = None

class PromptRequest(BaseModel):
    prompt: str

# ì „ë¬¸ê°€ ì‹œìŠ¤í…œ
class SimpleFashionExpertService:
    def __init__(self):
        self.client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        # ì „ë¬¸ê°€ë³„ íŠ¹ì„± ì •ì˜
        self.expert_profiles = {
            FashionExpertType.STYLE_ANALYST: {
                "role": "íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ë¶„ì„ ì „ë¬¸ê°€",
                "expertise": "ì²´í˜•ë¶„ì„, í•ê°ë¶„ì„, ì‹¤ë£¨ì—£",
                "focus": "ì‚¬ìš©ìì˜ ì²´í˜•ê³¼ ì–´ìš¸ë¦¬ëŠ” ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ê³  í•ê°ì„ ê³ ë ¤í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
                "prompt_template": "ì‚¬ìš©ìì—ê²Œ ì´ì „ í…œí”Œë¦¿ì´ ì¡´ì¬í•  ê²½ìš°ì—ë§Œ, ì´ì „ í…œí”Œë¦¿ì˜ ì¶”ì²œì´ ì–´ìš¸ë¦´ì§€ì— ëŒ€í•œ ìƒê°ì„ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ íšŒì˜ì  80% ê¸ì •ì  20%ì˜ ëŠë‚Œìœ¼ë¡œ 1ì¤„ê³¼, ì²´í˜•ë¶„ì„ê³¼ í•ê°ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê³ ë ¤í•˜ê³  ìˆìŒì„ ì•Œë ¤ì£¼ê³  ì–´ìš¸ë¦¬ëŠ” ì‹¤ë£¨ì—£ê³¼ ìŠ¤íƒ€ì¼ì„ í•œ ì¤„ ì´ë‚´ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ì„±ë³„ì€ ëª¨ë¥´ê² ìœ¼ë©´ ë¬¼ì–´ë³´ê±°ë‚˜, 20ëŒ€ ë‚¨ìë¡œ ê°€ì •í•˜ì„¸ìš”."
            },
            FashionExpertType.TREND_EXPERT: {
                "role": "íŒ¨ì…˜ íŠ¸ë Œë“œ ì „ë¬¸ê°€",
                "expertise": "ìµœì‹ íŠ¸ë Œë“œ, ì…€ëŸ½ìŠ¤íƒ€ì¼",
                "focus": "ìµœì‹  íŒ¨ì…˜ íŠ¸ë Œë“œ, ì¸í”Œë£¨ì–¸ì„œ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
                "prompt_template": "ì‚¬ìš©ìì—ê²Œ ì´ì „ í…œí”Œë¦¿ì˜ ì¶”ì²œì´ ì–´ìš¸ë¦´ì§€ì— ëŒ€í•œ ìƒê°ì„ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ íšŒì˜ì  80% ê¸ì •ì  20%ì˜ ëŠë‚Œìœ¼ë¡œ 1ì¤„ê³¼,í˜„ì¬ íŠ¸ë Œë“œì™€ ì…€ëŸ½ ìŠ¤íƒ€ì¼ì„ ê³ ë ¤í•˜ê³  ìˆìŒì„ ì•Œë ¤ì£¼ê³  íŠ¸ë Œë””í•œ ì˜·ì„ í•œ ì¤„ ì´ë‚´ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”."
            },
            # FashionExpertType.BUDGET_MANAGER: {
            #     "role": "íŒ¨ì…˜ ì˜ˆì‚° ê´€ë¦¬ ì „ë¬¸ê°€", 
            #     "expertise": "ê°€ê²©ë¹„êµ, ê°€ì„±ë¹„, íˆ¬ìê°€ì¹˜",
            #     "focus": "ì˜ˆì‚° ëŒ€ë¹„ ìµœì ì˜ ê°€ì„±ë¹„ì™€ ì¥ê¸°ì  íˆ¬ìê°€ì¹˜ë¥¼ ê³ ë ¤í•œ í˜„ì‹¤ì ì¸ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
            #     "prompt_template": "ì˜ˆì‚°ì„ ê³ ë ¤í•˜ì—¬ ê°€ì„±ë¹„ ì¢‹ê³  íˆ¬ìê°€ì¹˜ê°€ ë†’ì€ ì‹¤ìš©ì ì¸ ì˜·ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”."
            # },
            FashionExpertType.COLOR_EXPERT: {
                "role": "í¼ìŠ¤ë„ ì»¬ëŸ¬ ì „ë¬¸ê°€",
                "expertise": "í¼ìŠ¤ë„ì»¬ëŸ¬, ìƒ‰ìƒì¡°í•©, í†¤ì˜¨í†¤", 
                "focus": "ê°œì¸ì˜ í”¼ë¶€í†¤ê³¼ ì–´ìš¸ë¦¬ëŠ” ìƒ‰ìƒ ë¶„ì„ê³¼ ì¡°í™”ë¡œìš´ ì»¬ëŸ¬ ì¡°í•©ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                "prompt_template": "ì‚¬ìš©ìì—ê²Œ ì´ì „ í…œí”Œë¦¿ì˜ ì¶”ì²œì´ ì–´ìš¸ë¦´ì§€ì— ëŒ€í•œ ìƒê°ì„ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ íšŒì˜ì  80% ê¸ì •ì  20%ì˜ ëŠë‚Œìœ¼ë¡œ 1ì¤„ê³¼, í¼ìŠ¤ë„ ì»¬ëŸ¬ì™€ ìƒ‰ìƒ ì¡°í•©ì„ ê³ ë ¤í•˜ê³  ìˆìŒì„ ì•Œë ¤ì£¼ê³  ì–´ìš¸ë¦¬ëŠ” ìƒ‰ìƒì˜ ì˜·ì„ í•œ ì¤„ ì´ë‚´ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ì„±ë³„ê³¼ í”¼ë¶€í†¤ì„ ì‚¬ìš©ìê°€ ì•Œë ¤ì£¼ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ì¼ë°˜ì ì¸ 20ëŒ€ ë‚¨ìë¡œ ê°€ì •í•˜ê³ , ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”."
            },
            # FashionExpertType.TPO_EXPERT: {
            #     "role": "TPO ìƒí™©ë³„ íŒ¨ì…˜ ì „ë¬¸ê°€",
            #     "expertise": "ìƒí™©ë¶„ì„, ë“œë ˆìŠ¤ì½”ë“œ, ì˜ˆì˜ë²”ì ˆ",
            #     "focus": "ì‹œê°„, ì¥ì†Œ, ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ë“œë ˆìŠ¤ì½”ë“œì™€ ì˜ˆì˜ë¥¼ ê³ ë ¤í•œ íŒ¨ì…˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
            #     "prompt_template": "ìƒí™©ê³¼ ì¥ì†Œì— ë§ëŠ” ì ì ˆí•œ ë“œë ˆìŠ¤ì½”ë“œë¥¼ ê³ ë ¤í•˜ì—¬ ì˜ˆì˜ì— ë§ëŠ” ì˜·ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”."
            # },
            FashionExpertType.FITTING_COORDINATOR: {
                "role": "ê°€ìƒ í”¼íŒ… ì½”ë””ë„¤ì´í„°",
                "expertise": "í”¼íŒ…ì—°ë™, ê²°ê³¼ë¶„ì„, ëŒ€ì•ˆì œì‹œ",
                "focus": "ëª¨ë“  ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì½”ë””ë„¤ì´ì…˜ì„ ì™„ì„±í•©ë‹ˆë‹¤.",
                "prompt_template": "ì•ì„  ì „ë¬¸ê°€ë“¤ì˜ ì¡°ì–¸ì„ ì¢…í•©í•˜ì—¬ í‰ê°€ ì˜ê²¬ê³¼ ì™„ì„±ëœ ì½”ë””ë„¤ì´ì…˜ì„ í•œ ì¤„ ì´ë‚´ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”."
            }
        }
    
    async def get_single_expert_analysis(self, request: ExpertAnalysisRequest):
        """ë‹¨ì¼ ì „ë¬¸ê°€ ë¶„ì„"""
        expert_profile = self.expert_profiles[request.expert_type]
        
        # ì „ë¬¸ê°€ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context_parts = [f"ì‚¬ìš©ì ìš”ì²­: {request.user_input}"]
        
        if request.user_profile:
            context_parts.append(f"ì‚¬ìš©ì ì •ë³´: {request.user_profile}")
        
        if request.context_info:
            context_parts.append(f"ìƒí™© ì •ë³´: {request.context_info}")
        
        context_parts.append(f"\n{expert_profile['role']}ìœ¼ë¡œì„œ {expert_profile['prompt_template']}")
        
        expert_prompt = "\n\n".join(context_parts)
        
        # OpenAI í˜¸ì¶œ
        try:
            response = await self._call_openai_async(
                f"ë‹¹ì‹ ì€ {expert_profile['role']}ì…ë‹ˆë‹¤. {expert_profile['focus']} ì „ë¬¸ ì˜ì—­: {expert_profile['expertise']}",
                expert_prompt
            )
            
            return {
                "expert_type": request.expert_type.value,
                "expert_role": expert_profile["role"],
                "analysis": response,
                "expertise_areas": expert_profile["expertise"]
            }
            
        except Exception as e:
            logger.error(f"ì „ë¬¸ê°€ ë¶„ì„ ì‹¤íŒ¨ - {request.expert_type}: {e}")
            raise e
    
    async def get_expert_chain_analysis(self, request: ExpertChainRequest):
        """ì „ë¬¸ê°€ ì²´ì¸ ë¶„ì„"""
        expert_results = []
        accumulated_insights = []
        
        for expert_type in request.expert_sequence:
            # ì´ì „ ì „ë¬¸ê°€ë“¤ì˜ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
            current_context = request.context_info or {}
            if accumulated_insights:
                current_context["previous_expert_insights"] = accumulated_insights[-3:]  # ìµœê·¼ 3ê°œë§Œ
            
            expert_request = ExpertAnalysisRequest(
                user_input=request.user_input,
                room_id=request.room_id,
                expert_type=expert_type,
                user_profile=request.user_profile,
                context_info=current_context
            )
            
            expert_result = await self.get_single_expert_analysis(expert_request)
            expert_results.append(expert_result)
            
            # ë‹¤ìŒ ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸ ëˆ„ì 
            accumulated_insights.append({
                "expert": expert_type.value,
                "key_point": expert_result["analysis"][:100] + "..."  # ìš”ì•½ë§Œ
            })
        
        # ìµœì¢… ì¢…í•©
        return {
            "expert_analyses": expert_results,
            # "expert_count": len(expert_results),
            # "comprehensive_recommendation": self._synthesize_results(expert_results),
            # "collaboration_flow": accumulated_insights
        }
    
    def _synthesize_results(self, expert_results: List[Dict]) -> str:
        """ì „ë¬¸ê°€ ê²°ê³¼ ì¢…í•©"""
        synthesis = "===== ì¢…í•© íŒ¨ì…˜ ì¶”ì²œ =====\n\n"
        
        for result in expert_results:
            synthesis += f"ğŸ”¹ {result['expert_role']}: {result['analysis'][:150]}...\n\n"
        
        synthesis += "ğŸ“‹ ìµœì¢… ì¶”ì²œ: ëª¨ë“  ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ ì¢…í•©í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë‹¨ í•˜ë‚˜ì˜ ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ëŒ€ì•ˆ ì—†ì´."
        
        return synthesis
    
    async def _call_openai_async(self, system_prompt: str, user_prompt: str) -> str:
        """ë¹„ë™ê¸° OpenAI í˜¸ì¶œ"""
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            self._call_openai_sync,
            system_prompt,
            user_prompt
        )
        return response
    
    def _call_openai_sync(self, system_prompt: str, user_prompt: str) -> str:
        """ë™ê¸° OpenAI í˜¸ì¶œ"""
        response = self.client.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=settings.LLM_MAX_TOKENS,
            temperature=settings.LLM_TEMPERATURE
        )
        return response.choices[0].message.content

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
expert_service = SimpleFashionExpertService()

# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
def health_check():
    return {"status": "healthy", "message": "íŒ¨ì…˜ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ ì¤‘"}

@app.get("/api/test")
def test():
    return {"message": "Fashion Expert API Test", "experts": list(FashionExpertType)}

@app.post("/api/ask")
async def ask_single(request: PromptRequest):
    """ê¸°ì¡´ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ API (í˜¸í™˜ì„± ìœ ì§€)"""
    try:
        response = expert_service.client.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¢…í•© íŒ¨ì…˜ ì–´ë“œë°”ì´ì €ì…ë‹ˆë‹¤."},
                {"role": "user", "content": request.prompt}
            ],
            max_tokens=settings.LLM_MAX_TOKENS,
            temperature=settings.LLM_TEMPERATURE
        )
        return ResponseModel(data=response.choices[0].message.content)
    except Exception as e:
        logger.error(f"ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/expert/single")
async def single_expert_analysis(request: ExpertAnalysisRequest):
    """ë‹¨ì¼ ì „ë¬¸ê°€ ë¶„ì„"""
    try:
        result = await expert_service.get_single_expert_analysis(request)
        return ResponseModel(data=result)
    except Exception as e:
        logger.error(f"ë‹¨ì¼ ì „ë¬¸ê°€ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/expert/chain")
async def expert_chain_analysis(request: ExpertChainRequest):
    """ì „ë¬¸ê°€ ì²´ì¸ ë¶„ì„ - ëª¨ë“  ì „ë¬¸ê°€ê°€ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„"""
    try:
        result = await expert_service.get_expert_chain_analysis(request)
        return ResponseModel(data=result)
    except Exception as e:
        logger.error(f"ì „ë¬¸ê°€ ì²´ì¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/expert/types")
async def get_expert_types():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ê°€ íƒ€ì…ê³¼ ì„¤ëª…"""
    expert_info = {}
    for expert_type, profile in expert_service.expert_profiles.items():
        expert_info[expert_type.value] = {
            "role": profile["role"],
            "expertise": profile["expertise"],
            "focus": profile["focus"]
        }
    return ResponseModel(data=expert_info)

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ íë ˆì´ì…˜ API
@app.post("/api/curation")
async def generate_curation(request: ExpertChainRequest):
    """ê¸°ì¡´ íë ˆì´ì…˜ API - ì´ì œ ì „ë¬¸ê°€ ì²´ì¸ìœ¼ë¡œ ì²˜ë¦¬"""
    try:
        result = await expert_service.get_expert_chain_analysis(request)
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        converted_results = []
        for i, expert_result in enumerate(result["expert_analyses"]):
            converted_results.append({
                "style": expert_result["expert_type"], 
                "content": expert_result["analysis"] + f" ({i+1}ë²ˆì§¸ ì „ë¬¸ê°€)"
            })
        
        return ResponseModel(data={
            "room_id": request.room_id,
            "results": converted_results,
            "comprehensive_analysis": result["comprehensive_recommendation"]
        })
    except Exception as e:
        logger.error(f"íë ˆì´ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    logger.info("ğŸƒâ€â™‚ï¸ íŒ¨ì…˜ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘... í¬íŠ¸ 6020")
    uvicorn.run("main_simple_experts:app", host="0.0.0.0", port=6020, reload=True)